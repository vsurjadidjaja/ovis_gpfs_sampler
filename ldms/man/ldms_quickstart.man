.\" Manpage for LDMS_QuickStart
.\" Contact ovis-help@ca.sandia.gov to correct errors or typos.
.TH man 7 "12 Dec 2016" "v3" "LDMS_QuickStart man page"

.SH NAME
LDMS_QuickStart - man page for Quick Start of LDMS

.SH INTRODUCTION
LDMS is the Lightweight Distributed Metric Service. LDMS is a distributed data collection, transport, and storage tool that supports a wide variety of configuration options.
There are three main functional components described below.
.PP
.I
Samplers
run one or more plugins that periodically sample data of interest.
Each plugin defines a group of metrics called a metric set.
The sampling frequency is user defined and can be dynamically changed.
A host can simultaneously run multiple plugins.
Configuration flags determine whether the sampling plugins run synchronously or asynchonously
(both on a host and across hosts). Memory allocated for a particular metric set is overwritten by each
successive sampling. The host daemon does not retain sample history;
plugins do not typically retain history, but can be written to do so.
.PP
.I
Aggregators
collect data in a pull fashion from samplers
and/or other aggregators. The collection frequency
is user defined and operates independently of other
collection operations and sampling operations. Distinct metric
sets can be collected at different frequencies. Once started, the aggregation schedule cannot
be altered without restarting the aggregator. Fan-in refers to
the number of hosts collected from by a single aggregator.
Maximum fan-in varies by transport but is roughly
9,000:1 for the socket transport and for the RDMA
transport over Infiniband. It is > 15000:1 for RDMA over
the Cray Gemini transport. Daisy chaining  is not limited to two levels;
multiple aggregators may aggregate from the same sampler or aggregator ldmsd.
Fan-in at higher levels is limited
by the aggregator host capabilities (CPU, memory, network
bandwidth, and storage bandwidth).
.PP
.I
Storage
plugins write in a variety of formats.
Comma Separated Value (CSV) file storage of metric sets
plugins are provided. Storage occurs when a
valid updated metric set data is collected by an aggregator that
has been configured to write that data to storage. Collection of
a metric set whose data has not been updated or is incomplete
does not result in a write to storage in any format.

.PP
The host daemon is the same base code in all cases; differentiation is based on configuration of plugins for sampling or
storage and on configuring aggregation of data from other host daemons.


.SH DESCRIPTION
Quick Start instructions for LDMS (Lightweight Distributed Metric Service).
.PP
This man page describes how to configure and run LDMS daemons (ldmsd) to perform the following tasks:
.IP \[bu]
collect data
.IP \[bu]
aggregate data from multiple ldmsds
.IP \[bu]
store collected data to files.
.PP
There are three basic configurations that will be addressed:
.IP \[bu]
configuring an ldmsd with collector plugins
.IP \[bu]
configuring a ldmsd to aggregate information from other ldmsds
.IP \[bu]
configuring a store_csv storage plugin on an ldmsd.
.PP
The order in which these configurations should be performed does not matter with respect to collectors and aggregators.
.PP
While a complete listing of flags and parameters can be seen by running
ldmsd and the configuration tools with the --help directive, this document describes the flags and parameters required for running a basic setup.
.PP
There are no run scripts provided in the current release; the examples here can be used in the creation of such.
.PP

.SH Arrangement of this document
This document is arranged as follows:
.IP
1) Prerequisites
.IP
2) Build and install
.IP
3) Configuring and Starting an ldmsd (general)
.IP
4) through 8) Example ldmsd configurations and queries
.IP
9) Protection Domain Tags (Cray Only)
.IP
10) Troubleshooting
.IP
11) About the man pages
.PP


.SH 1) PREREQUISITES
.PP
.IP \[bu]
All sections below assume the build directory is /tmp/opt/ovis.
.IP \[bu]
libevent-2.0 is a requirement.
.IP \[bu]
Python 2.7 or Python 2.6 with the argparse module is required for ldmsd_controller

.SH 2) BUILD/INSTALL:
.PP
There is a separate document with build/install instructions.

.PP
The default ldms build in v3 has authentication turned on.
This document does not include use of the authentication flags; the instructions here
are as if you had built with --disable_ovis_auth. For more information on authentication,
see the ldms_authentication man page.

.SH 3) CONFIGURING AND STARTING AN LDMSD

.SS 3-1) Environment Variables for LDMS
You will need to set the following environment variables when running LDMS daemons.
This assumes that ldms has been installed in to /tmp/opt/ovis.
.nf
.RS
export LD_LIBRARY_PATH=/tmp/opt/ovis/lib/:/tmp/opt/ovis/lib/ovis-ldms/:<path to libevent-2.0>/lib:$LD_LIBRARY_PATH
export ZAP_LIBPATH=/tmp/opt/ovis/lib/ovis-ldms
export LDMSD_PLUGIN_LIBPATH=/tmp/opt/ovis/lib/ovis-ldms
export PATH=/tmp/opt/ovis/sbin/:/tmp/opt/ovis/bin:$PATH
export LDMSD_SOCKPATH=/tmp/run/ldmsd
.RE
.fi
.PP
LDMSD_SOCKPATH determines the location for the unix domain socket (described in the ldmsd args below).
The default is /var/run/ldmsd. Make sure you use a location that is writeable if you are running as non-root.

.SS 3-2) Options for Configuring Plugins of an ldmsd
.PP
Plugins for an ldmsd can be configured via a configuration file specified as an argument to the "-c"
flag. Also, ldmsd_controller is a configuration tool that can work in interactive mode
and can also can be directed commands/scripts to a socket. The plugin configuration commands
are the same in all cases.

.PP
In the instructions below, we briefly illustrate use of the configuration
script to ldmsd vs ldmsd_controller. Some environmental variables have been
supressed in this section for clarity. In all subsequent examples (Sections 4+), we provide
versbose detail for the ldmsd configuration script method only. Altering
this to use the other methods should then be obvious.

.SS
3-2a) Configuring an ldmsd via a configuration script

This is the most usual mode of configuring ldms in production scenarios
and can also be used for test scenarios.

Example commands for configuring a sampler:
.nf
.RS
> more config.file

load name=meminfo
config name=meminfo producer=vm1_1 instance=vm1_1/meminfo
start name=meminfo interval=1000000
.RE
.fi

The path to the configuration script is then provided to the ldmsd via the "-c" flag when it is started:

Example ldmsd start command with a configuration script:
.nf
.RS
ldmsd -x sock:60000 -S tmp/ldmsd/sock1 -l /tmp/log/logfile -v DEBUG -c ./config.file
.RE
.fi

.SS
3-2b) Configuring ldmsd via ldmsd_controller
You can use ldmsd_controller to connect to the ldmsd at any time to issue plugin commands.
This is most often used for dynamically issuing commands to a running ldmsd.

Example ldmsd start command without a configuration script:
.nf
.RS
ldmsd -x sock:60000 -S tmp/ldmsd/sock1 -l /tmp/log/logfile -v DEBUG
.RE
.fi

Call the ldmsd_controller interactively and enter the same commands
as you would in the configuration script.
.nf
.RS
ldmsd_controller --host vm1_1 --port=61000
ldmsd_controller> load name=meminfo
ldmsd_controller> config name=meminfo producer=vm1_1 instance=vm1_1/meminfo
ldmsd_controller> start name=meminfo interval=1000000
ldmsd_controller> quit
.RE
.fi

Relatedly, you can run ldmsd_controller with the commands in script form.
For example:

.nf
.RS
> more config.sh

#!/bin/bash
echo "load name=meminfo"
echo "config name=meminfo producer=vm1_1 instance=vm1_1/meminfo"
echo "start name=meminfo interval=1000000"
.RE
.fi

Call the ldmsd_controller with the script:
.nf
.RS
ldmsd_controller --host vm1_1 --port=60000 --script ./config.sh
.RE
.fi

ldmsd_contoller may be executed multiple times to issues different
commands to the same ldmsd.

.SS 3-3) Starting an ldmsd
.PP
3-3a) Set environment variables, as described above.
.PP
3-3b) Run ldmsd:
.nf
.RS
<path to executable>/ldmsd -x <transport>:<listen port> -S <unix domain socket path/name> -l <log file path/name> -v <LOG_LEVEL> -c config.file
.RE
.fi

Notes:
.IP \[bu]
Transport is one of: sock, rdma, ugni (ugni is Cray specific for using RDMA over the Gemini/Aries network)
.IP \[bu]
The configuration file contains the commands to configure the plugins.
.IP \[bu]
The unix domain socket can be used to communicate configuration information to an ldmsd.
The default path for this is /var/run/ldmsd/. To change this the environment variable LDMSD_SOCKPATH
must be set to the desired path (e.g. export LDMSD_SOCKPATH=/tmp/run/ldmsd)
.IP \[bu]
No log can be can be obtained by using LOG_LEVEL QUIET, or specifying /dev/null for the log file,
or using command line redirection.
.IP \[bu]
The default is to run as a background process but the -F flag can be specified for foreground
.IP \[bu]
A script can be made to start ldmsd and collectors on a host where that script contains the information to execute the command.

.PP
3-3c) Examples for launching ldmsd:

.IP \[bu]
Start an ldmsd on the socket transport with a log file and a configuration file.
.nf
.RS
/tmp/opt/ovis/sbin/ldmsd -x sock:60000 -S /var/run/ldmsd/metric_socket -l /tmp/opt/ovis/logs/1 -c config.file
.RE
.ni

.IP \[bu]
Same but with log level QUIET
.nf
.RS
/tmp/opt/ovis/sbin/ldmsd -x sock:60000 -S /var/run/ldmsd/metric_socket -l /tmp/opt/ovis/logs/1 -c config.file -V QUIET
.RE
.fi

.IP \[bu]
Start 2 instances of ldmsd on host vm1
.nf
.RS
Note: Make sure to use different socket names and listen on different ports if you are on the same host.
/tmp/opt/ovis/sbin/ldmsd -x sock:60000 -S /var/run/ldmsd/metric_socket_vm1_1 -l /tmp/opt/ovis/logs/vm_1 -c config.file
/tmp/opt/ovis/sbin/ldmsd -x sock:60001 -S /var/run/ldmsd/metric_socket_vm1_2 -l /tmp/opt/ovis/logs/vm_2 -c config.file
.RE
.fi

.SH 4) EXAMPLE: CONFIGURE AN LDMSD WITH SAMPLER PLUGINS
.SS 4-1) Create the configuration file for the sampler plugins:
.PP
Configure a "meminfo" collector plugin to collect every second.
.nf
.RS
load name=meminfo
config name=meminfo producer=vm1_1 instance=vm1_1/meminfo
start name=meminfo interval=1000000
.RE
.ni

.PP
Notes:
.IP \[bu]
For synchronous operation include "offset=<#usec>" in start line (e.g. start name=meminfo interval=xxx offset=yyy).
This will cause the sampler to target interval + yyy aligned to the second and micro second
(e.g. every 5 seconds with an offset of 0 usec would ideally result in collections at 00:00:00, 00:00:05, 00:00:10, etc.
whereas with an offset of 100,000 usec it would be 00:00:00.1, 00:00:05.1, 00:00:10.1, etc)
.IP \[bu]
Different plugins may have additional configuration parameters.
.PP

.SS 4-2) Set environment variables, as described above.
.SS 4-3) Start the ldmsd with the config file, as described above. e.g.,
.RS
ldmsd -x sock:60000 -S tmp/ldmsd/sock1 -l /tmp/log/logfile -v DEBUG -c ./config.file
.RE

.SS 4-4) Verifying the collector
.PP
At this point the ldmsd collector should be checked using the utility ldms_ls
(See Using ldms_ls below)


.SH 5) EXAMPLE: CONFIGURE AN AGGREGATOR USING LDMSD_CONTROLLER
.SS 5-1) Start 2 separate ldmsds, one on host vm1_1 and one on host vm1_2, with sampler plugins, as described above
.SS 5-2) Write a script to add producers and start collecting from them:

This adds vm1_1 as a producer with its sets collected at 2 second intervals
and vm1_2 as a producer with its sets collected at 5 second intervals. Here the "name" of the producer
must match the "producer" name given to the sampler.

The first set of lines adds the producers. The second set of lines establishes the aggregation from them.
at the specified intervals.
.PP
.nf
> more add_prdcr.config
prdcr_add name=vm1_2 host=vm1 type=active xprt=sock port=60001 interval=20000000
prdcr_start name=vm1_2
prdcr_add name=vm1_1 host=vm1 type=active xprt=sock port=60000 interval=20000000
prdcr_start name=vm1_1
updtr_add name=policy2_h1 interval=2000000 offset=0
updtr_prdcr_add name=policy2_h1 regex=vm1_1
updtr_start name=policy2_h1
updtr_add name=policy5_h2 interval=5000000 offset=0
updtr_prdcr_add name=policy5_h2 regex=vm1_2
updtr_start name=policy5_h2
.fi

.SS 5-3) Set environment variables, as described above
.SS 5-4) Start an ldmsd on your host to aggregate using the configuration file
.RS
/tmp/opt/ovis/sbin/ldmsd -x sock:60002 -S /var/run/ldmsd/metric_socket_agg -l /tmp/opt/ovis/logs/vm1_agg -c ./add_prdcr.sh
.RE

.PP
Notes:
.IP \[bu]
There is no requirement that aggregator intervals match collection intervals
.IP \[bu]
Because the collection and aggregation processes operate asynchronously there is the potential for duplicate data collection as well as missed samples.
The first is handled by the storage plugins by comparing generation numbers and not storing duplicates. The second implies either a loss
in fidelity (if collecting counter data) or a loss of data points here and there (if collecting differences of counter values or non counter
values). This can be handled using the synchronous option on both collector and aggregator but is not covered here.
.fi

.SS 5-4) At this point the ldmsd collector should be checked using the utility ldms_ls
(See Using ldms_ls below). In this case you should see metric sets for both vm1_1 and vm1_2 displayed when you query the aggregator ldmsd using ldms_ls.


.SH 6) EXAMPLE: CONFIGURE AN LDMS AGGREGATOR WITH A STORAGE PLUGIN
.SS 6-1) Add storage configuration lines to the configuration file described above.
This adds a store_csv to store sets whose schema are meminfo or vmstat and
whose instance name matches the regex. A set's schema and instance names will be seen
in the output of ldms_ls (described below).
.PP
> more add_store.sh
load name=store_csv
config name=store_csv path=<<STORE_PATH>> action=init altheader=0 rollover=30 rolltype=1
strgp_add name=policy_mem plugin=store_csv container=csv schema=meminfo
strgp_prdcr_add name=policy_mem regex=vm*
strgp_start name=policy_vmstat
strgp_add name=policy_vmstat plugin=store_csv container=csv schema=vmstat
strgp_prdcr_add name=policy_vmstat regex=vm*
strgp_start name=policy_vmstat
.PP

Notes:
.IP \[bu]
For the csv store, the whole path must pre-exist.
.IP \[bu]
See the Plugin_store_csv man page for more info on the plugin configuration arguments.
.IP \[bu]
If you want to collect on a host and store that data on the same host, run two ldmsd's: one with a collector plugin only and one as an aggegrator with a store plugin only.
.PP

.SS 6-2) Set environment variables, as described above
.SS 6-3) Start the aggregator with the full configuration file (both aggregator and store lines), as described above
.SS 6-4) Verify the store
Go to data store and verify files have been created and are being written to
.nf
.RS
cd <<STORE_PATH>>/<container>
ls -ltr
.RE
.fi
You can now utilize this data.
.PP
Data will flush to the store when the OS flushes data unless an advanced flag is used. Thus,
in a default configuration, if you have a small number of nodes and/or a long interval,
you may not see data appear in the store for a few minutes.


.SH 7) EXAMPLES: USING LDMS_LS TO DISPLAY SETS/METRICS FROM AN LDMSD
.SS 7-1) Set environment variables, as described above
.SS 7-2a) Query ldmsd on host vm1 listening on port 60000 (sampler) using the sock transport for metric sets being served by that ldmsd
.nf
.RS
ldms_ls -h vm1 -x sock -p 60000
Should return:
vm1_1/meminfo
vm1_1/vmstat
.RE
.fi

.SS 7-2b) Query ldmsd on host vm1 listening on port 60002 (aggregator) using the sock transport for metric sets being served by that ldmsd
.nf
.RS
ldms_ls -h vm1 -x sock -p 60002
Should return:
vm1_1/meminfo
vm1_1/vmstat
vm1_2/meminfo
vm1_2/vmstat
.RE
.fi

.SS 7-2c) Query ldmsd on host vm1 listening on port 60000 using the sock transport for the names and contents of metric sets being served by that ldmsd.
Should return: Set names (vm1_1/meminfo and vm1_1/vmstat in this case) as well as all names and values associated with each set respectively.
Only vm1_1/meminfo shown here.
.nf
.RS
> ldms_ls -h vm1 -x sock -p 60000 -l
vm1_1/meminfo: consistent, last update: Wed Jul 31 21:51:08 2013 [246540us]
U64 33084652         MemTotal
U64 32092964         MemFree
U64 0                Buffers
U64 49244            Cached
U64 0                SwapCached
U64 13536            Active
U64 39844            Inactive
U64 5664             Active(anon)
U64 13540            Inactive(anon)
U64 7872             Active(file)
U64 26304            Inactive(file)
U64 2996             Unevictable
U64 2988             Mlocked
U64 0                SwapTotal
U64 0                SwapFree
U64 0                Dirty
U64 0                Writeback
U64 7164             AnonPages
U64 6324             Mapped
U64 12544            Shmem
U64 84576            Slab
U64 3948             SReclaimable
U64 80628            SUnreclaim
U64 1608             KernelStack
U64 804              PageTables
U64 0                NFS_Unstable
U64 0                Bounce
U64 0                WritebackTmp
U64 16542324         CommitLimit
U64 73764            Committed_AS
U64 34359738367      VmallocTotal
U64 3467004          VmallocUsed
U64 34356268363      VmallocChunk
U64 0                HugePages_Total
U64 0                HugePages_Free
U64 0                HugePages_Rsvd
U64 0                HugePages_Surp
U64 2048             Hugepagesize
U64 565248           DirectMap4k
U64 5726208          DirectMap2M
U64 27262976         DirectMap1G
.RE
.nf

.SH 7-2d) Query for a non-existent set:
.nf
.RS
ldms_ls -h vm1 -x sock -p 60000 -l vm1_1/foo
ldms_ls: No such file or directory
ldms_ls: lookup failed for set 'vm1_1/foo'
.RE
.fi

.SH 7-2e) Display metadata about sets contained by vm1 ldmsd listening on port 60000
.nf
.RS
ldms_ls -h vm1 -x sock -p 60000 -v
vm1_1/meminfo: consistent, last update: Fri Dec 16 17:12:08 2016 [5091us]
  METADATA --------
    Producer Name : vm1_1
    Instance Name : vm1_1/meminfo
      Schema Name : meminfo
             Size : 1816
     Metric Count : 43
               GN : 2
  DATA ------------
        Timestamp : Fri Dec 16 17:12:08 2016 [5091us]
         Duration : [0.000072s]
       Consistent : TRUE
             Size : 384
               GN : 985
  -----------------
.RE
.fi
.PP

.SH 8) STOP AN LDMSD
.SS
To kill all ldmsd on a host
.nf
.RS
killall ldmsd
.RE
.ni

.SH 9) PROTECTION DOMAIN TAGS (Cray)
.SS 9-1) Cray XE/XK:
If you are going to be using the "ugni" transport (RDMA over Gemini) you will need to run with either system (as root) or user (as user) ptags. While root CAN run using any ptag the fact that its use is unknown to ALPS could cause collisions with applications.

.SS To see current ptags:
.nf
.RS
> apstat -P
PDomainID           Type    Uid   PTag     Cookie
LDMS              system      0     84 0xa9380000
.RE
.ni

.SS To create a userspace ptag:
.nf
.RS
apmgr pdomain -c <somenamehere>

Example:
> apmgr pdomain -c foo
> apstat -P
PDomainID           Type    Uid   PTag     Cookie
LDMS              system      0     84 0xa9380000
foo                 user     12345  233 0xa1230000
.RE
.fi
Note: A system administrator will have to setup system ptags and/or enable users to set up ptags.

.SS To remove a userspace ptag:
.nf
.RS
apmgr pdomain -r <somenamehere>
.RE
.fi
Note: The userid of the ptag being removed must match that of the user running the command or root

.SS PTAG-Related Enviroment variables for ldms (XE/XK)
Set the following environment variables for either user or system ptags (example shows user ptag values):
.nf
.RS
export ZAP_UGNI_PTAG 233
export ZAP_UGNI_COOKIE 0xa1230000
.RE
.fi

.SS Starting ldms from aprun with ptags
When running with user space ptags you must specify the ptag name when using aprun
.nf
.RS
aprun <<usual aprun args here>> -p foo ldmsd <<usual ldmsd flags here>>
or
aprun <<usual aprun args here>> -p foo ldms_ls <<usual ldms_ls flags here>>
.RE
.fi
Note: On some systems you will run aprun after a qsub -I or within a script specified in qsub or similiar.


.SS 9-2) Cray XC, CLE <= 5.2:
If you are going to be using the "ugni" transport (RDMA over Aries) you will need to run with either system (as root) or user (as user) ptags. While root CAN run using any ptag the fact that its use is unknown to ALPS could cause collisions with applications.

.SS To see current ptags:
.nf
.RS
> apstat -P
PDomainID   Type   Uid     Cookie    Cookie2
LDMS      system     0 0x86b80000          0
.RE
.ni

.SS To create a userspace ptag:
.nf
.RS
apmgr pdomain -c <somenamehere>

Example:
> apmgr pdomain -c foo
> apstat -P
PDomainID   Type   Uid     Cookie    Cookie2
LDMS      system     0 0x86b80000          0
foo         user 20596 0x86bb0000 0x86bc0000
.RE
.fi
Note: A system administrator will have to setup system ptags and/or enable users to set up ptags.

.SS To remove a userspace ptag:
.nf
.RS
apmgr pdomain -r <somenamehere>
.RE
.fi
Note: The userid of the ptag being removed must match that of the user running the command or root

.SS PTAG-Related Enviroment variables for ldms (XC)
Set the following environment variables.
On XC the ptag value doesn't matter but ZAP_UGNI_PTAG must be defined.
Set the Cookie (not Cookie2) for either user or system ptag.
.nf
.RS
export ZAP_UGNI_PTAG=0
export ZAP_UGNI_COOKIE=0x86bb0000
.RE
.fi

.SS Starting ldms from aprun with ptags
When running with user space ptags you must specify the ptag name when using aprun
.nf
.RS
aprun <<usual aprun args here>> -p foo ldmsd <<usual ldmsd flags here>>
or
aprun <<usual aprun args here>> -p foo ldms_ls <<usual ldms_ls flags here>>
.RE
.fi
Note: On some systems you will run aprun after a qsub -I or within a script specified in qsub or similiar.

.SH 10) TROUBLESHOOTING

.SS What causes the following error: libibverbs: Warning: RLIMIT_MEMLOCK is 32768 bytes?
Running as a user with "max locked memory" set too low.
The following is an example of trying to run ldms_ls as a user with "max locked memory" set to 32k:
.nf
.RS
ldms_ls -h <hostname> -x rdma -p <portnum>
libibverbs: Warning: RLIMIT_MEMLOCK is 32768 bytes.
   This will severely limit memory registrations.
RDMA: recv_buf reg_mr failed: error 12
ldms_ls: Cannot allocate memory
.RE
.ni

.SS Why doesn't my ldmsd start?
.PP
Possible options:
.IP \[bu]
Check for existing /var/run/ldms/metric_socket or similar. Sockets can be left if an ldmsd did not clean up upon termination. kill -9 may leave the socket hanging around.
.IP \[bu]
The port you are trying to use may already be in use on the node. The following shows the logfile output of such a case:
.nf
.RS
Tue Sep 24 08:36:54 2013: Started LDMS Daemon version 2.1.0
Tue Sep 24 08:36:54 2013: Process 123456 listening on transport ugni:60020
Tue Sep 24 08:36:54 2013: EV_WARN: Can't change condition callbacks once they have been initialized.
Tue Sep 24 08:36:54 2013: Error 12 listening on the 'ugni' transport.
Tue Sep 24 08:36:54 2013: LDMS Daemon exiting...status 7
.RE
.ni
.IP \[bu]
If using the -l flag make sure that your log directory exists prior to running
.IP \[bu]
If writing to a store with this particular lmdsd make sure that your store directory exists prior to running
.IP \[bu]
If you are running on a Cray with transport ugni using a user space PTag, check that you called aprun with the -p flag
.RS
aprun -N 1 -n <number of nodes> -p <ptag name> run_my_ldmsd.sh
.RE
.RE

.SS How can I find what process is using the port?
.RS
netstat -abno
.RE

.SS Why arent all my hosts/sets adding to the aggregator?
Possible options:
.\.IP \[bu]
.\Running multiples on the same host from a script. Sometimes multiple ldmsctls running concurrently may collide in creating ports. They should clean up after themselves and this usually isn't an issue.
.IP \[bu]
use -m flag on the aggregator to use more memory when adding a lot of hosts
.IP \[bu]
use -p on the aggregator to use more processors
.SE

.SS Why isn't my ldmsd storing its own set to the store?
Currently, this is not supported. You can use a separate ldmsd on the same host to gather another ldmsd's data for that host.

.SS Why is my aggregator not responding (CRAY XE/XK)?
Running a ldmsd aggregator as a user but trying to aggregate from a ldmsd that uses a system ptag can result in the aggregator hanging (alive but not responding and not writing to the store). The following is the logfile output of such an aggregator:
.nf
.RS
Tue Sep 24 08:42:40 2013: Connected to host 'nid00081:60020'
Tue Sep 24 08:42:42 2013: cq_thread_proc: Error 11  monitoring the CQ.
.RE
.fi

.SH 11) MAN PAGES
ldms comes with man pages. In the build process these will be installed in <build_path>/ovis/share/man.
Man pages are in the following catagories:
.SS General
General pages address information, such as ldms_build_install, ldms_quickstart, and ldms_authentication.
.SS Utilities
Utilities pages address the various utilities and commands such as ldmsd, ldmsd_controller, and ldms_ls.
.SS Plugins
Plugin pages address all plugins, both samplers and stores. Naming convention for these pages is Plugin_XXX.
For example: Plugin_aries_mmr, Plugin_cray_system_sampler_variants, Plugin_kgnilnd, Plugin_meminfo,
Plugin_procinterrupts, Plugin_procnetdev, Plugin_procnfs, Plugin_store_csv, Plugin_store_function_csv,
Plugin_store_sos, and Plugin_vmstat.

.SH NOTES
As part of the install, test scripts are placed in /tmp/opt/ovis/bin. These scripts
may serve as additional examples. These are being converted from using
the obsolete ldmsctl tool to the ldmsd_controller tool, so they may not be
fully updated at any given time.

.SH BUGS
No known bugs.


.SH SEE ALSO
ldms_build_install(7), ldmsd(8), ldmsd_controller(8), ldms_authentication(7), ldms_build_install(7), ldms_ls(8)
